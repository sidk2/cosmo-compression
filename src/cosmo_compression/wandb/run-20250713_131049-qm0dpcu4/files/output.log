Using 14000 training points and 1000 validation points.
Using 14000 training samples and 1000 validation samples (suite=Astrid, data=LH).
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/6
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 6 processes
----------------------------------------------------------------------------------------------------

/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/sid/cosmo_compression/src/cosmo_compression/final_test exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]

  | Name    | Type          | Params | Mode
--------------------------------------------------
0 | encoder | ResNetEncoder | 2.9 M  | train
1 | decoder | FlowMatching  | 52.9 M | train
--------------------------------------------------
55.8 M    Trainable params
0         Non-trainable params
55.8 M    Total params
223.143   Total estimated model params size (MB)
543       Modules in train mode
0         Modules in eval mode
Epoch 1:  35%|██▊     | 51/146 [01:35<02:57,  0.53it/s, v_num=pcu4, train_loss=0.559, val_loss=0.569]
/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['train_loss', 'lr-AdamW', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?
                                                                                                     
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/sid/cosmo_compression/src/cosmo_compression/train.py", line 227, in <module>
[rank0]:     train(args)
[rank0]:   File "/home/sid/cosmo_compression/src/cosmo_compression/train.py", line 158, in train
[rank0]:     trainer.fit(
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 561, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 47, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 599, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1012, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1056, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank0]:     self.advance()
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 320, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 176, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1302, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 123, in optimizer_step
[rank0]:     return optimizer.step(closure=closure, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank0]:     loss = closure()
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 109, in _wrap_closure
[rank0]:     closure_result = closure()
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
[rank0]:     self._result = self.closure(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in closure
[rank0]:     self._backward_fn(step_output.closure_loss)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 241, in backward_fn
[rank0]:     call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 328, in _call_strategy_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py", line 213, in backward
[rank0]:     self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py", line 73, in backward
[rank0]:     model.backward(tensor, *args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py", line 1097, in backward
[rank0]:     loss.backward(*args, **kwargs)
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/sid/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 251.75 MiB is free. Process 3728507 has 15.08 GiB memory in use. Including non-PyTorch memory, this process has 26.75 GiB memory in use. Process 3820715 has 494.00 MiB memory in use. Process 3820717 has 494.00 MiB memory in use. Process 3820718 has 494.00 MiB memory in use. Process 3820716 has 494.00 MiB memory in use. Process 3820714 has 494.00 MiB memory in use. Of the allocated memory 24.23 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
